{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_Type_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmLF8gvu4x1O"
      },
      "source": [
        "from sklearn import  preprocessing, linear_model, naive_bayes, metrics, svm,model_selection,decomposition, ensemble\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        " \n",
        "import numpy as np\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSD9PeRY87Uy"
      },
      "source": [
        "dataset=pd.read_csv('/content/drive/MyDrive/data1.txt',names=['question'])\n",
        "\n",
        "new= dataset[\"question\"].str.split(\":\",expand=True)\n",
        "    \n",
        "x = new[1]\n",
        "y= new[0]\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x, y,random_state=42,stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzwaiAv69EL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36847ed-4571-41df-8784-b4309a02b8c7"
      },
      "source": [
        "#word-level\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "tfidf_vect.fit(x)\n",
        "word_xtrain =  tfidf_vect.transform(train_x)\n",
        "word_xvalid =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "\n",
        "le=LabelEncoder()\n",
        "le.fit(y)\n",
        "train_y=le.transform(train_y)\n",
        "valid_y=le.transform(valid_y)\n",
        "\n",
        "print(train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 6 6 ... 3 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAJlnYoH9IHN"
      },
      "source": [
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(2,3))\n",
        "tfidf_vect_ngram.fit(x)\n",
        "ngram_xtrain =  tfidf_vect_ngram.transform(train_x)\n",
        "ngram_xvalid=  tfidf_vect_ngram.transform(valid_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KseLiPC49LRn"
      },
      "source": [
        "# characters level tf-idf\n",
        "tfidf_vect_char = TfidfVectorizer(analyzer='char',ngram_range=(2,3))\n",
        "tfidf_vect_char.fit(x)\n",
        "char_xtrain =  tfidf_vect_char.transform(train_x) \n",
        "char_xvalid =  tfidf_vect_char.transform(valid_x) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC1uFH3c9OWr"
      },
      "source": [
        "# count vectorizer object \n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(x)\n",
        "count_xtrain=  count_vect.transform(train_x)\n",
        "count_xvalid =  count_vect.transform(valid_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFuMyqsM8_Dv"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier=classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    return classifier,metrics.accuracy_score(predictions, valid_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCVb91fM-MWT",
        "outputId": "42fb8d8f-f1a3-4283-d545-bbe7304249de"
      },
      "source": [
        "#Word Level\n",
        "\n",
        "\n",
        "NB_word,accuracy = train_model(naive_bayes.MultinomialNB(), word_xtrain, train_y, word_xvalid)\n",
        "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "LR_word,accuracy = train_model(linear_model.LogisticRegression(), word_xtrain, train_y, word_xvalid)\n",
        "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# SVM on Ngram Level TF IDF Vectors\n",
        "SVM_word,accuracy = train_model(svm.SVC(), word_xtrain, train_y, word_xvalid)\n",
        "print (\"SVM,  WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "RF_word,accuracy = train_model(ensemble.RandomForestClassifier(),word_xtrain, train_y, word_xvalid)\n",
        "print (\"RF, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "#decision tree\n",
        "DT_word,accuracy = train_model(DecisionTreeClassifier(), word_xtrain, train_y, word_xvalid)\n",
        "print (\"DT, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "#mlp\n",
        "MLP_word,accuracy = train_model(MLPClassifier(hidden_layer_sizes=(50),activation = 'relu',random_state=1), word_xtrain, train_y,word_xvalid)\n",
        "print (\"mlp, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, WordLevel TF-IDF:  0.7313432835820896\n",
            "LR, WordLevel TF-IDF:  0.817910447761194\n",
            "SVM,  WordLevel TF-IDF:  0.8\n",
            "RF, WordLevel TF-IDF:  0.808955223880597\n",
            "DT, WordLevel TF-IDF:  0.7134328358208956\n",
            "mlp, WordLevel TF-IDF:  0.808955223880597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU4hNNKm-tYI",
        "outputId": "d675d299-2a66-431d-9eab-31c5cda81e9a"
      },
      "source": [
        "#NGram level\n",
        "\n",
        "NB_ngram,accuracy = train_model(naive_bayes.MultinomialNB(), ngram_xtrain, train_y, ngram_xvalid)\n",
        "print (\"NB, ngramLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier \n",
        "LR_ngram,accuracy = train_model(linear_model.LogisticRegression(), ngram_xtrain, train_y, ngram_xvalid)\n",
        "print (\"LR, ngramLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# SVM \n",
        "SVM_ngram,accuracy = train_model(svm.SVC(), ngram_xtrain, train_y, ngram_xvalid)\n",
        "print (\"SVM,  ngramLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# RF \n",
        "RF_ngram,accuracy = train_model(ensemble.RandomForestClassifier(), ngram_xtrain, train_y,ngram_xvalid)\n",
        "print (\"RF, ngramLevel TF-IDF: \", accuracy)\n",
        "\n",
        "#decision tree\n",
        "DT_ngram,accuracy = train_model(DecisionTreeClassifier(), ngram_xtrain, train_y, ngram_xvalid)\n",
        "print (\"DT, ngramLevel TF-IDF: \", accuracy)\n",
        "\n",
        "MLP_ngram,accuracy = train_model(MLPClassifier(hidden_layer_sizes=(50),activation = 'relu',random_state=1),ngram_xtrain, train_y,ngram_xvalid)\n",
        "print (\"mlp, ngramLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, ngramLevel TF-IDF:  0.7343283582089553\n",
            "LR, ngramLevel TF-IDF:  0.7134328358208956\n",
            "SVM,  ngramLevel TF-IDF:  0.7223880597014926\n",
            "RF, ngramLevel TF-IDF:  0.764179104477612\n",
            "DT, ngramLevel TF-IDF:  0.7134328358208956\n",
            "mlp, ngramLevel TF-IDF:  0.7164179104477612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hxUsM-W_Dft",
        "outputId": "611de942-4f64-4e09-c670-b12aacad8982"
      },
      "source": [
        "#Character Level\n",
        " \n",
        "NB_char,accuracy = train_model(naive_bayes.MultinomialNB(), char_xtrain, train_y, char_xvalid)\n",
        "print (\"NB, characterLevel TF-IDF: \", accuracy)\n",
        " \n",
        " \n",
        "LR_char,accuracy = train_model(linear_model.LogisticRegression(),char_xtrain, train_y,char_xvalid)\n",
        "print (\"LR,character TF-IDF: \", accuracy)\n",
        " \n",
        "SVM_char,accuracy = train_model(svm.SVC(),char_xtrain, train_y, char_xvalid)\n",
        "print (\"SVM, characterTF-IDF: \", accuracy)\n",
        " \n",
        " \n",
        "RF_char,accuracy = train_model(ensemble.RandomForestClassifier(), char_xtrain, train_y, char_xvalid)\n",
        "print (\"RF, characterTF-IDF: \", accuracy)\n",
        " \n",
        "DT_char,accuracy = train_model(DecisionTreeClassifier(), char_xtrain, train_y, char_xvalid)\n",
        "print (\"DT,characterTF-IDF: \", accuracy)\n",
        " \n",
        "MLP_char,accuracy = train_model(MLPClassifier(hidden_layer_sizes=(50),activation = 'relu',random_state=1),char_xtrain, train_y,char_xvalid)\n",
        "print (\"mlp, CharLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, characterLevel TF-IDF:  0.7731343283582089\n",
            "LR,character TF-IDF:  0.8238805970149253\n",
            "SVM, characterTF-IDF:  0.8388059701492537\n",
            "RF, characterTF-IDF:  0.844776119402985\n",
            "DT,characterTF-IDF:  0.7522388059701492\n",
            "mlp, CharLevel TF-IDF:  0.8686567164179104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VantA0Dk_WX0",
        "outputId": "044aa9fa-2959-47ea-b428-8cfe12e7a163"
      },
      "source": [
        " #Count Vectorizer\n",
        " \n",
        " #Naive Bayes \n",
        "NB_count,accuracy = train_model(naive_bayes.MultinomialNB(), count_xtrain, train_y, count_xvalid)\n",
        "print (\"NB, countvec: \", accuracy)\n",
        "\n",
        "# Linear Classifier \n",
        "LR_count,accuracy = train_model(linear_model.LogisticRegression(), count_xtrain, train_y,count_xvalid)\n",
        "print (\"LR,countvec: \", accuracy)\n",
        "\n",
        "# SVM \n",
        "SVM_count,accuracy = train_model(svm.SVC(),count_xtrain, train_y,count_xvalid)\n",
        "print (\"SVM, countvec: \", accuracy)\n",
        "\n",
        "# RF \n",
        "RF_count,accuracy = train_model(ensemble.RandomForestClassifier(),count_xtrain, train_y,count_xvalid)\n",
        "print (\"RF,countvec: \", accuracy)\n",
        "\n",
        "#decision tree\n",
        "DT_count,accuracy = train_model(DecisionTreeClassifier(),count_xtrain, train_y,count_xvalid)\n",
        "print (\"DT, countvec: \", accuracy)\n",
        "\n",
        "#mlp\n",
        "MLP_count,accuracy = train_model(MLPClassifier(hidden_layer_sizes=(50),activation = 'relu',random_state=1),count_xtrain, train_y,count_xvalid)\n",
        "print (\"mlp, Countvec: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, countvec:  0.7671641791044777\n",
            "LR,countvec:  0.7880597014925373\n",
            "SVM, countvec:  0.7492537313432835\n",
            "RF,countvec:  0.7791044776119403\n",
            "DT, countvec:  0.7164179104477612\n",
            "mlp, Countvec:  0.746268656716418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qXwB9ykKqth"
      },
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "trainLabels = encoder.fit_transform(train_y)\n",
        "trainLabels = [np_utils.to_categorical(i, num_classes=8) for i in trainLabels]\n",
        "trainLabels = np.asarray(trainLabels)\n",
        "\n",
        "validLabels = encoder.fit_transform(valid_y)\n",
        "validLabels = [np_utils.to_categorical(i, num_classes=8) for i in validLabels]\n",
        "validLabels = np.asarray(validLabels)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4XFmbWQPgoB"
      },
      "source": [
        "# Using FastText pre trained telugu embeddings\n",
        "embeddings_index = {}\n",
        "import numpy as np\n",
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "for i, line in enumerate(open('/content/drive/MyDrive/cc.te.300.vec',encoding=\"utf-8\")):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDciwfJMMzk_"
      },
      "source": [
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(x)\n",
        "word_index = token.word_index\n",
        "\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x),maxlen=32)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x),maxlen=32)\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
        "\n",
        "for word,i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)    # checking that particular indexed word in telugu embedding .vec file\n",
        "        if embedding_vector is not None:                 # if it is found in that .vec file  \n",
        "            embedding_matrix[i] = embedding_vector \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ9hAwKZKzV9"
      },
      "source": [
        "def create_model(model):\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((32, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
        "\n",
        "    if model==1:\n",
        "      #Add the convolutional layer\n",
        "       conv_layer = layers.Convolution1D(256, 3, activation=\"tanh\")(embedding_layer)\n",
        "    \n",
        "      #Add the pooling layer\n",
        "       layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "    elif model==2:  \n",
        "       layer = layers.LSTM(100)(embedding_layer)\n",
        "     \n",
        "    elif model==3:\n",
        "      layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    elif model==4:\n",
        "       layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(128, activation=\"tanh\")(layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxk2r139LkUr"
      },
      "source": [
        "cnn=create_model(model=1)\n",
        "lstm=create_model(model=2)\n",
        "gru=create_model(model=3)\n",
        "birnn=create_model(model=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4do0UT6NLk5Y",
        "outputId": "42a282df-d993-4d4a-af36-ba28de6034a4"
      },
      "source": [
        "cnn.fit(train_seq_x, trainLabels, epochs=10)\n",
        "\n",
        "predictions = cnn.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print('cnn ',acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 16s 26ms/step - loss: 1.7545 - accuracy: 0.3928\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 1.0840 - accuracy: 0.7000\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.7632 - accuracy: 0.8060\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.5463 - accuracy: 0.8469\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.4588 - accuracy: 0.8746\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.3411 - accuracy: 0.9134\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.2445 - accuracy: 0.9500\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.1664 - accuracy: 0.9694\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.1195 - accuracy: 0.9756\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.1030 - accuracy: 0.9877\n",
            "cnn  0.8417910447761194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NycP31hfLvcw",
        "outputId": "6aa70199-6fbf-4dc5-e377-f986a1618163"
      },
      "source": [
        "lstm.fit(train_seq_x, trainLabels, epochs=10)\n",
        "\n",
        "predictions = lstm.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print('LSTM ',acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 8s 49ms/step - loss: 1.8840 - accuracy: 0.3614\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 2s 48ms/step - loss: 1.3112 - accuracy: 0.5906\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 2s 47ms/step - loss: 0.9561 - accuracy: 0.7141\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 2s 47ms/step - loss: 0.7694 - accuracy: 0.7606\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 2s 47ms/step - loss: 0.6259 - accuracy: 0.8159\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 2s 47ms/step - loss: 0.5702 - accuracy: 0.8356\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 2s 48ms/step - loss: 0.4986 - accuracy: 0.8451\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 2s 47ms/step - loss: 0.4548 - accuracy: 0.8587\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 2s 48ms/step - loss: 0.3757 - accuracy: 0.8948\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 2s 48ms/step - loss: 0.3913 - accuracy: 0.8881\n",
            "LSTM  0.8388059701492537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v63SnZULvfp",
        "outputId": "ecfd212b-baa2-4b82-e1b5-fd904e6f665f"
      },
      "source": [
        "gru.fit(train_seq_x, trainLabels, epochs=10)\n",
        "\n",
        "predictions = gru.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print('GRU ',acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 5s 41ms/step - loss: 1.8508 - accuracy: 0.4445\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 1.1250 - accuracy: 0.6614\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.8552 - accuracy: 0.7423\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.7079 - accuracy: 0.7956\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.5624 - accuracy: 0.8258\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.4809 - accuracy: 0.8524\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.4587 - accuracy: 0.8512\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.4200 - accuracy: 0.8691\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.3574 - accuracy: 0.8927\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.3995 - accuracy: 0.8784\n",
            "GRU  0.8477611940298507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0JFKSVFLviQ",
        "outputId": "79700e45-d37d-42a6-8c33-1783b00eaba3"
      },
      "source": [
        "birnn.fit(train_seq_x, trainLabels, epochs=10)\n",
        "\n",
        "predictions = birnn.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print('BI RNN ',acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 9s 73ms/step - loss: 1.8080 - accuracy: 0.4046\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 1.0675 - accuracy: 0.6791\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.7697 - accuracy: 0.7741\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 2s 74ms/step - loss: 0.6926 - accuracy: 0.7823\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.5788 - accuracy: 0.8268\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.5209 - accuracy: 0.8499\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 2s 74ms/step - loss: 0.4469 - accuracy: 0.8656\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.4295 - accuracy: 0.8566\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.3980 - accuracy: 0.8847\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.3376 - accuracy: 0.9054\n",
            "BI RNN  0.8417910447761194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDl2kx0aLvk4",
        "outputId": "5e79756a-9fbb-4a4d-8b3d-f21039dc28f8"
      },
      "source": [
        "input1=['“జై జవాన్ జై కిసాన్” నినాదాన్ని ఎవరు రూపొందించారు?']\n",
        "valid_seq1 = sequence.pad_sequences(token.texts_to_sequences(input1),maxlen=32)\n",
        "\n",
        "predict=cnn.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('CNN ',le.inverse_transform(predict))\n",
        "\n",
        "predict=lstm.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('LSTM ',le.inverse_transform(predict))\n",
        "\n",
        "predict=gru.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('GRU ',le.inverse_transform(predict))\n",
        "\n",
        "predict=birnn.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('BI RNN ',le.inverse_transform(predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN  ['PERS']\n",
            "LSTM  ['PERS']\n",
            "GRU  ['PERS']\n",
            "BI RNN  ['PERS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRFNyvg-S48p",
        "outputId": "ed775bd2-6bee-453e-840a-daf4a383ec92"
      },
      "source": [
        "word_xvalid1 =  tfidf_vect.transform(input1)\n",
        "\n",
        "print(\"NB Word level\",le.inverse_transform(NB_word.predict(word_xvalid1)))\n",
        "print(\"LR Word level\",le.inverse_transform(LR_word.predict(word_xvalid1)))\n",
        "print(\"DT Word level\",le.inverse_transform(DT_word.predict(word_xvalid1)))\n",
        "print(\"RF Word level\",le.inverse_transform(RF_word.predict(word_xvalid1)))\n",
        "print(\"MLP Word level\",le.inverse_transform(MLP_word.predict(word_xvalid1)))\n",
        "\n",
        "word_xvalid1 =  tfidf_vect_ngram.transform(input1)\n",
        "\n",
        "print(\"\\nNB Ngram level\",le.inverse_transform(NB_ngram.predict(word_xvalid1)))\n",
        "print(\"LR Ngram level\",le.inverse_transform(LR_ngram.predict(word_xvalid1)))\n",
        "print(\"DT Ngram level\",le.inverse_transform(DT_ngram.predict(word_xvalid1)))\n",
        "print(\"RF Ngram level\",le.inverse_transform(RF_ngram.predict(word_xvalid1)))\n",
        "print(\"MLP Ngram level\",le.inverse_transform(MLP_ngram.predict(word_xvalid1)))\n",
        "\n",
        "\n",
        "word_xvalid1 =  tfidf_vect_char.transform(input1)\n",
        "\n",
        "print(\"\\nNB Char level\",le.inverse_transform(NB_char.predict(word_xvalid1)))\n",
        "print(\"LR Char level\",le.inverse_transform(LR_char.predict(word_xvalid1)))\n",
        "print(\"DT Char level\",le.inverse_transform(DT_char.predict(word_xvalid1)))\n",
        "print(\"RF Char level\",le.inverse_transform(RF_char.predict(word_xvalid1)))\n",
        "print(\"MLP Char level\",le.inverse_transform(MLP_char.predict(word_xvalid1)))\n",
        "\n",
        "\n",
        "word_xvalid1 =  count_vect.transform(input1)\n",
        "\n",
        "print(\"\\nNB CountVector\",le.inverse_transform(NB_count.predict(word_xvalid1)))\n",
        "print(\"LR Count vector\",le.inverse_transform(LR_count.predict(word_xvalid1)))\n",
        "print(\"DT Count vector\",le.inverse_transform(DT_count.predict(word_xvalid1)))\n",
        "print(\"RF Count vector\",le.inverse_transform(RF_count.predict(word_xvalid1)))\n",
        "print(\"MLP Count vector\",le.inverse_transform(MLP_count.predict(word_xvalid1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB Word level ['PERS']\n",
            "LR Word level ['PERS']\n",
            "DT Word level ['PERS']\n",
            "RF Word level ['PERS']\n",
            "MLP Word level ['PERS']\n",
            "\n",
            "NB Ngram level ['PERS']\n",
            "LR Ngram level ['PERS']\n",
            "DT Ngram level ['PERS']\n",
            "RF Ngram level ['PERS']\n",
            "MLP Ngram level ['PERS']\n",
            "\n",
            "NB Char level ['PERS']\n",
            "LR Char level ['PERS']\n",
            "DT Char level ['PERS']\n",
            "RF Char level ['PERS']\n",
            "MLP Char level ['PERS']\n",
            "\n",
            "NB CountVector ['PERS']\n",
            "LR Count vector ['PERS']\n",
            "DT Count vector ['PERS']\n",
            "RF Count vector ['PERS']\n",
            "MLP Count vector ['PERS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir2tdBpredRg"
      },
      "source": [
        "pip freeze>req.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}