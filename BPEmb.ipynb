{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BPEmb_word_ALL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkYJPZkMoomi"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import numpy as np\n",
        "import pandas, numpy, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from sklearn import model_selection, svm\n",
        "import pandas as pd\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = open('data.txt',encoding=\"utf-8\").read()\n",
        "labels, texts = [], []\n",
        "\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split(\":\")\n",
        "    labels.append(content[0])\n",
        "    texts.append(\" \".join(content[1:]))\n",
        "    \n",
        "\n",
        "\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'],random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "trainLabels = encoder.fit_transform(train_y)\n",
        "trainLabels = [np_utils.to_categorical(i, num_classes=8) for i in trainLabels]\n",
        "trainLabels = np.asarray(trainLabels)\n",
        "\n",
        "\n",
        "validLabels = encoder.fit_transform(valid_y)\n",
        "validLabels = [np_utils.to_categorical(i, num_classes=8) for i in validLabels]\n",
        "validLabels = np.asarray(validLabels)\n",
        "\n",
        "# Using FastText pre trained telugu embeddings\n",
        "embeddings_index = {}\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "for i, line in enumerate(open('/content/drive/MyDrive/te.wiki.bpe.vs200000.d300.w2v.txt',encoding=\"utf-8\")):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x),maxlen=32)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x),maxlen=32)\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
        "\n",
        "for word,i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)    # checking that particular indexed word in telugu embedding .vec file\n",
        "        if embedding_vector is not None:                 # if it is found in that .vec file  \n",
        "            embedding_matrix[i] = embedding_vector \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgGuonV7-m3Q",
        "outputId": "7230bc51-bfeb-4cb7-b512-d147b297686e"
      },
      "source": [
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((32, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    \n",
        "\n",
        "    #Add the convolutional layer\n",
        "    conv_layer = layers.Convolution1D(256, 3, activation=\"relu\")(embedding_layer)\n",
        "    \n",
        "    #Add the pooling layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "    \n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(128, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "cnn = create_cnn()\n",
        "\n",
        "cnn.fit(train_seq_x, trainLabels, epochs=10)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = cnn.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "\n",
        "print(predictions1)\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 2s 26ms/step - loss: 1.6404 - accuracy: 0.4251\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 1s 29ms/step - loss: 0.6854 - accuracy: 0.8401\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 1s 28ms/step - loss: 0.3930 - accuracy: 0.9113\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.2134 - accuracy: 0.9533\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1201 - accuracy: 0.9806\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 1s 28ms/step - loss: 0.0758 - accuracy: 0.9935\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.0669 - accuracy: 0.9866\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.0361 - accuracy: 0.9966\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 1s 27ms/step - loss: 0.0384 - accuracy: 0.9928\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 1s 27ms/step - loss: 0.0318 - accuracy: 0.9960\n",
            "[3 3 3 3 6 3 6 1 2 3 0 3 1 6 0 3 3 3 3 6 3 6 2 3 1 5 1 1 6 3 3 6 6 1 7 1 5\n",
            " 2 3 3 6 1 3 3 1 6 6 3 6 2 3 1 3 3 0 1 6 3 3 1 0 1 0 3 6 1 3 1 6 1 0 6 1 1\n",
            " 1 1 3 3 3 1 3 3 1 3 6 6 1 0 0 3 3 6 3 6 3 6 6 3 3 3 3 6 6 3 2 6 1 3 3 6 3\n",
            " 3 3 6 6 6 2 5 1 3 6 3 3 1 6 1 5 6 1 3 3 0 3 3 3 6 0 3 1 1 1 2 3 1 0 3 3 3\n",
            " 0 3 3 3 0 1 1 3 5 3 1 1 3 2 3 1 6 0 6 3 1 0 1 6 0 2 6 3 3 0 6 3 1 0 3 1 3\n",
            " 0 0 6 3 1 1 1 3 1 3 6 0 3 3 3 3 1 1 3 6 1 3 1 6 7 6 1 3 3 1 1 3 6 6 6 3 1\n",
            " 6 3 1 6 1 6 1 6 6 1 6 3 3 0 1 1 3 3 3 3 3 1 3 0 1 2 3 3 0 3 3 7 3 6 1 6 1\n",
            " 2 1 3 6 0 3 6 2 3 1 3 3 6 1 3 3 3 3 6 6 3 3 6 6 3 6 3 0 1 3 3 3 3 3 3 6 1\n",
            " 3 6 3 3 3 3 3 3 6 3 1 6 6 1 3 1 6 0 3 3 6 6 3 1 1 2 6 2 3 0 6 1 6 3 6 6 3\n",
            " 0 3]\n",
            "0.826865671641791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnjNzYXIowsV",
        "outputId": "02bc7c1e-dc54-49ac-9c88-2fccd20698f6"
      },
      "source": [
        "#RNN Lstm glove \n",
        "\n",
        "def create_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((32, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    \n",
        "\n",
        "    \n",
        "        # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "    \n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(128, activation=\"tanh\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "lstm = create_rnn()\n",
        "\n",
        "lstm.fit(train_seq_x, trainLabels, epochs=10)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = lstm.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "\n",
        "print(predictions1)\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print(acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 5s 47ms/step - loss: 1.6958 - accuracy: 0.4896\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 1s 45ms/step - loss: 0.8725 - accuracy: 0.7590\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 2s 47ms/step - loss: 0.5481 - accuracy: 0.8523\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 1s 47ms/step - loss: 0.3338 - accuracy: 0.9028\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 1s 46ms/step - loss: 0.2104 - accuracy: 0.9446\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 2s 48ms/step - loss: 0.1833 - accuracy: 0.9497\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 2s 48ms/step - loss: 0.1120 - accuracy: 0.9695\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 2s 50ms/step - loss: 0.1022 - accuracy: 0.9771\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 2s 51ms/step - loss: 0.1004 - accuracy: 0.9725\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 1s 45ms/step - loss: 0.0547 - accuracy: 0.9914\n",
            "[3 3 3 3 6 3 6 1 2 3 0 3 1 6 0 3 3 3 3 6 3 6 5 3 1 5 1 0 6 3 3 6 6 1 7 1 3\n",
            " 2 3 3 6 1 3 3 1 6 6 3 6 2 3 1 3 3 0 4 6 0 3 1 0 1 0 3 6 1 3 1 0 1 0 6 1 6\n",
            " 1 1 3 3 3 1 3 3 1 3 6 6 1 0 0 3 3 1 3 6 3 6 6 3 3 3 3 6 6 3 2 6 3 3 3 6 3\n",
            " 3 1 6 6 6 2 5 1 3 6 3 3 1 6 1 3 6 0 3 3 0 3 3 3 6 0 3 4 1 1 2 3 1 0 3 3 3\n",
            " 0 3 3 3 0 1 1 3 3 3 1 3 3 2 3 1 6 0 6 3 1 1 1 6 0 5 6 1 3 0 6 3 1 0 3 1 3\n",
            " 0 0 6 3 1 1 3 3 1 3 1 0 3 3 3 3 1 1 3 6 1 3 1 1 7 6 1 3 3 1 4 3 6 3 6 3 1\n",
            " 6 3 1 6 1 6 1 6 6 1 0 3 3 0 1 1 3 3 3 3 6 1 3 0 0 2 3 3 0 3 3 7 3 6 1 6 1\n",
            " 2 1 3 6 0 3 1 2 3 1 3 3 6 1 3 3 3 3 6 0 3 3 6 6 3 6 1 0 1 3 3 3 3 3 2 6 1\n",
            " 3 6 3 3 3 3 3 3 6 3 1 6 6 1 3 1 6 0 3 3 6 6 3 1 1 2 6 2 3 0 6 1 3 3 6 6 3\n",
            " 0 3]\n",
            "0.8328358208955224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z2bFMHnpLCg",
        "outputId": "ee4801ca-467a-4e2d-a40a-f73c4d5ae689"
      },
      "source": [
        "#RNN GRU glove\n",
        "\n",
        "def create_rnngru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((32, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        " \n",
        "\n",
        "    \n",
        " # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "    \n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(128, activation=\"tanh\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "gru = create_rnngru()\n",
        "\n",
        "gru.fit(train_seq_x, trainLabels, epochs=10)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = gru.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "\n",
        "print(predictions1)\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print(acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 4s 39ms/step - loss: 1.6497 - accuracy: 0.4941\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 1s 42ms/step - loss: 0.7520 - accuracy: 0.7856\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.5117 - accuracy: 0.8573\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 1s 42ms/step - loss: 0.3608 - accuracy: 0.9005\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.2559 - accuracy: 0.9333\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.1897 - accuracy: 0.9468\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 1s 38ms/step - loss: 0.1230 - accuracy: 0.9622\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 1s 43ms/step - loss: 0.1202 - accuracy: 0.9676\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.0653 - accuracy: 0.9868\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 2s 78ms/step - loss: 0.0483 - accuracy: 0.9945\n",
            "[3 3 6 3 6 3 6 1 2 3 0 3 1 6 0 3 3 3 3 6 3 6 2 3 1 5 1 0 6 3 3 6 6 1 7 1 5\n",
            " 2 3 3 6 1 3 3 1 6 6 3 6 2 3 1 3 3 0 4 6 0 3 1 0 1 0 3 6 1 3 1 1 1 0 6 1 6\n",
            " 1 1 3 3 3 1 3 3 1 3 6 6 1 0 0 3 3 1 3 6 3 6 6 3 3 3 3 6 6 3 2 0 1 3 3 6 3\n",
            " 3 1 6 6 6 5 5 1 3 6 3 3 1 6 1 5 6 1 1 3 0 3 3 3 6 0 3 4 1 1 2 3 1 0 3 3 3\n",
            " 0 3 3 3 0 1 1 3 5 3 1 3 3 2 3 1 6 0 6 3 1 1 1 6 0 5 6 1 3 0 6 3 1 6 3 1 3\n",
            " 0 0 6 3 1 1 6 3 1 3 1 0 3 3 3 3 1 1 3 6 1 3 1 6 7 6 1 1 3 1 1 5 6 2 6 3 1\n",
            " 6 3 1 6 1 6 1 6 6 1 6 3 3 0 1 1 1 3 3 3 6 1 3 0 0 2 3 3 0 3 3 7 3 6 1 6 1\n",
            " 2 1 3 1 0 3 1 5 3 1 3 3 6 1 3 3 3 1 6 6 3 3 6 6 3 6 1 0 1 1 3 3 3 3 2 6 1\n",
            " 3 6 3 3 3 3 3 3 6 6 1 6 6 1 3 1 6 0 3 1 6 6 3 1 1 2 6 2 3 0 6 1 3 3 6 6 3\n",
            " 0 3]\n",
            "0.844776119402985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XseoVw3gpOrQ",
        "outputId": "411ca42d-056f-4783-f7b2-5a99b0bccdc6"
      },
      "source": [
        "#Bidirectional RNN glove\n",
        "\n",
        "def create_brnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((32, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    \n",
        "\n",
        "    \n",
        " # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "    \n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(128, activation=\"tanh\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "biRNN = create_brnn()\n",
        " \n",
        "biRNN.fit(train_seq_x, trainLabels, epochs=10)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = biRNN.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "\n",
        "print(predictions1)\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print(acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 7s 72ms/step - loss: 1.6695 - accuracy: 0.4326\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 2s 74ms/step - loss: 0.8329 - accuracy: 0.7772\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 2s 69ms/step - loss: 0.4988 - accuracy: 0.8607\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 2s 67ms/step - loss: 0.3611 - accuracy: 0.8922\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 2s 70ms/step - loss: 0.2716 - accuracy: 0.9203\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 2s 69ms/step - loss: 0.1873 - accuracy: 0.9520\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 2s 69ms/step - loss: 0.1269 - accuracy: 0.9671\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 0.1013 - accuracy: 0.9787\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 0.0714 - accuracy: 0.9892\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 2s 65ms/step - loss: 0.0579 - accuracy: 0.9908\n",
            "[3 3 6 3 6 3 6 1 2 3 0 3 1 6 0 3 3 3 3 6 3 6 2 3 1 5 1 0 6 3 3 6 6 1 7 1 5\n",
            " 1 3 3 6 1 3 3 1 6 6 3 6 2 3 1 3 3 0 1 6 0 3 1 0 1 0 3 6 1 3 1 1 1 0 6 1 6\n",
            " 1 1 6 3 3 1 3 3 1 3 6 6 1 0 0 3 3 1 3 6 3 6 6 3 3 3 3 6 6 3 2 6 1 3 3 6 3\n",
            " 3 1 6 6 6 2 5 1 3 6 3 3 1 6 1 5 6 1 1 3 0 3 3 3 6 0 3 1 1 1 2 3 1 0 3 3 3\n",
            " 0 3 3 3 0 1 1 3 5 3 1 3 3 3 3 1 6 0 6 3 1 6 1 6 0 5 6 1 3 0 6 3 1 6 3 1 3\n",
            " 0 0 6 3 1 1 6 3 1 3 1 0 3 6 3 3 1 1 3 6 1 3 1 1 7 6 1 1 3 1 4 2 6 0 6 3 1\n",
            " 6 3 1 6 1 6 1 6 6 1 6 3 3 0 1 1 1 3 3 3 6 1 3 0 0 2 3 3 0 3 3 7 3 6 1 6 1\n",
            " 2 1 3 6 0 3 1 2 3 6 3 1 6 1 3 3 3 3 6 0 3 3 6 6 3 6 5 0 1 3 3 3 3 3 5 6 1\n",
            " 3 6 3 3 3 3 3 3 6 3 1 6 6 1 3 1 6 0 3 1 6 6 3 1 1 2 6 2 3 0 6 1 3 3 6 6 3\n",
            " 0 3]\n",
            "0.8417910447761194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZN7rDnPqGpJ",
        "outputId": "07410433-269f-4c80-bebb-1a9d83166f51"
      },
      "source": [
        "#RCNN\n",
        "\n",
        "\n",
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((32, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "\n",
        "\n",
        "    \n",
        "# Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "    \n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(128, activation=\"tanh\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(8, activation=\"softmax\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "rcnn = create_rcnn()\n",
        " \n",
        "rcnn.fit(train_seq_x, trainLabels, epochs=10)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "predictions = rcnn.predict(valid_seq_x)\n",
        "predictions1 = predictions.argmax(axis=-1)\n",
        "validLabels1 = validLabels.argmax(axis=-1)\n",
        "\n",
        "\n",
        "print(predictions1)\n",
        "acc = metrics.accuracy_score(predictions1, validLabels1)\n",
        "\n",
        "print(acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 1.7745 - accuracy: 0.3273\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.9958 - accuracy: 0.7187\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6400 - accuracy: 0.8107\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.4276 - accuracy: 0.8840\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.2860 - accuracy: 0.9398\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.2130 - accuracy: 0.9562\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.1605 - accuracy: 0.9603\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.1553 - accuracy: 0.9483\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1203 - accuracy: 0.9518\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.1071 - accuracy: 0.9675\n",
            "[0 1 3 6 6 6 3 6 6 3 6 3 0 1 1 6 3 6 0 3 3 3 1 6 3 6 6 3 1 0 6 1 3 3 0 3 1\n",
            " 3 1 1 6 6 1 3 1 3 1 0 3 3 3 1 3 1 1 3 3 3 6 1 3 3 3 6 3 6 3 0 1 3 1 0 3 3\n",
            " 3 0 3 2 3 0 6 3 3 1 6 1 6 3 3 1 3 3 6 6 0 3 1 3 3 6 3 6 6 0 0 1 0 3 1 3 0\n",
            " 6 6 3 1 1 6 3 1 6 3 3 1 1 3 1 3 3 1 0 3 1 1 6 3 1 3 7 6 6 6 2 6 3 1 6 3 5\n",
            " 6 6 2 2 5 3 3 3 3 1 6 6 1 3 3 6 1 3 1 6 3 6 3 1 6 1 6 3 2 1 6 3 6 0 1 1 1\n",
            " 3 3 3 3 0 3 3 1 1 3 3 6 3 1 3 3 3 3 6 3 3 3 1 1 6 1 6 3 1 7 1 6 6 6 0 1 3\n",
            " 6 3 3 3 6 1 1 6 3 3 0 0 0 6 6 3 3 3 6 1 2 3 6 6 1 3 6 1 6 6 3 3 1 1 0 7 0\n",
            " 6 6 1 0 1 6 3 2 6 0 6 6 4 6 0 6 1 6 1 1 6 6 0 6 3 6 3 3 3 3 1 0 6 1 6 3 1\n",
            " 1 3 3 6 1 1 1 3 3 3 3 1 6 6 1 3 7 1 6 2 3 3 1 6 6 0 6 6 3 1 3 1 3 1 3 6 1\n",
            " 3 1]\n",
            "0.7373134328358208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE5VptZkFLOG",
        "outputId": "4e60cee6-4855-4de6-a288-b9139730a504"
      },
      "source": [
        "input1=['భారతదేశ జనాభా ఎంత? ']\n",
        "valid_seq1 = sequence.pad_sequences(token.texts_to_sequences(input1),maxlen=32)\n",
        "\n",
        "predict=cnn.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('CNN ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=lstm.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('LSTM ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=gru.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('GRU ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=biRNN.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('BI RNN ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=rcnn.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('RCNN ',encoder.inverse_transform(predict))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN  ['NUMB']\n",
            "LSTM  ['NUMB']\n",
            "GRU  ['NUMB']\n",
            "BI RNN  ['NUMB']\n",
            "RCNN  ['NUMB']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEhw0z18FVeg",
        "outputId": "bb578f58-181d-47da-ba89-825fbb4971ae"
      },
      "source": [
        "input1=['మధ్యప్రదేశ్ రాజధాని ఏమిటి?']\n",
        "valid_seq1 = sequence.pad_sequences(token.texts_to_sequences(input1),maxlen=32)\n",
        "\n",
        "predict=cnn.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('CNN ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=lstm.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('LSTM ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=gru.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('GRU ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=biRNN.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('BI RNN ',encoder.inverse_transform(predict))\n",
        "\n",
        "predict=rcnn.predict(valid_seq1)\n",
        "predict = predict.argmax(axis=-1)\n",
        "print('RCNN ',encoder.inverse_transform(predict))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN  ['LOCA']\n",
            "LSTM  ['LOCA']\n",
            "GRU  ['LOCA']\n",
            "BI RNN  ['LOCA']\n",
            "RCNN  ['LOCA']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}